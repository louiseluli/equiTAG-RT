# 2025-09-03 — Reboot baseline & DB sanity

## Why

Tie the new repo to CE901 criteria (reproducibility, documentation, data integrity) and RQs 1–3: understand current categorisation data, quantify what’s available, and prepare for fairness measurement.

## What

- Created `equiTAG-RT` skeleton.
- Copied SQLite database and vendored legacy collectors.
- Added `src/setup/verify_database.py` to emit:
  - `reports/metrics/v0_db_profile.json`
  - `reports/metrics/v0_sample_videos.csv`

## How to run

python -m src.setup.verify_database
Outputs (expected)
JSON with tables: videos, video_tags, video_categories, category_status, collection_state, audit_terms (+ counts, indices, FKs).
CSV sample with 100 videos and aggregated tags.
Next
Lock config loader & seed policy; integrate protected-terms lexicon.
Build the RQ1 evidence pack: distributions, co-occurrence, overlap (titles vs tags), subgroup parity views.

### `docs/decisions/ADR-001-repo-structure.md`

```markdown
# ADR-001: Repository Structure & First Artifact Strategy

**Status:** Accepted — 2025-09-03

**Context.** We must meet CE901 standards: replicable pipeline, clear docs, fair ML evaluation. Baseline proof of data integrity is required before modeling.

**Decision.** Use a conventional, analysis-first structure:

- `src/setup` for integrity/bootstrap tools.
- `reports/metrics` as the single sink for machine artifacts (CSV/JSON).
- Vendored legacy collectors under `src/collect` to be refactored later.

**Consequences.**

- Easy narrative: Data → Integrity → EDA → Modeling → Fairness → Mitigation.
- Artifacts ready for dissertation figures/tables; minimal friction to proceed.

## Config & Reproducibility Module

- Added `src/utils/config_loader.py`:
  - Loads YAML config and resolves paths relative to project root.
  - Enforces **seed=75** and prints a consistent run header (root, config, DB, seed, device).
  - Picks device with safe defaults (MPS > CUDA > CPU) and applies precision settings.
- This aligns with CE901 marking (replicability, clarity) and supports RQs 2–4 by ensuring all modeling runs are traceable and comparable.

## Verify DB now uses shared config/seed/device

- Refactored `src/setup/verify_database.py` to import `src.utils.config_loader`.
- Standardised run header and deterministic seed across setup steps.
- Artifacts now resolved via `cfg.paths.metrics` (single source of truth).
- This ensures all experiments and setup steps are consistent and reproducible, as required by CE901 and the dissertation RQs.

## Protected lexicon loader

- Added `src/utils/lexicon_loader.py`
  - Loads `config/protected_terms.json`, validates structure, compiles boundary-safe regex (`edge` guards).
  - CLI: `python -m src.utils.lexicon_loader --audit` writes `reports/metrics/v0_lexicon_audit.json`.
  - Supports wildcards `*` and multi-word phrases.
- Rationale: consistent subgroup attribution for RQ1–RQ4; reduces false positives and documents overlaps.
```

## Lexicon loader hardened

- Replaced `src/utils/lexicon_loader.py`:
  - Skips metadata keys (e.g., `_source_notes`) in JSON.
  - Coerces single strings to lists; warns and skips invalid subgroups.
  - Supports `*` wildcard → `[\w\-]*`; boundary-aware regex (`\b...\b`).
  - `--audit` writes `reports/metrics/v0_lexicon_audit.json`.
- Aligns with CE901 reproducibility and RQ1 (valid, auditable subgroup detection).

## RQ1 Evidence Pack

- Implemented `src/analysis/01_rq1_categorisation_evidence.py`:
  - Coverage by subgroup across **titles** vs **tags**.
  - Outcome summaries per subgroup (views & rating).
  - Overlap (Jaccard) matrices within namespaces.
  - Figures exported in **light** and **dark** styles (no colormaps).
- Outputs power the Results & Discussion for RQ1 and set baselines for RQ2–RQ3.
- Aligned with CE901 (clear documentation, replicable analysis) and RQ1 (understand current categorisation data).

## RQ1 Narrative Tables

- Added `src/analysis/01b_rq1_tables_report.py` to merge coverage & outcomes into thesis-ready tables.
- Exports consolidated CSV + per-namespace CSV, Markdown, and LaTeX.
- Complements light/dark figures; no colormap usage.

- Supports CE901 clarity and RQ1 narrative (data availability and subgroup outcomes).

Interpretive pointers (use in thesis)
Tilt (tags > title / title > tags / balanced): skews in platform labelling that may signal discoverability or editorial emphasis.
Union_n / rating_mean / views_median: approximate exposure and reception by subgroup.
Overlap matrices: potential confounds or co-labelling patterns across subgroups.

## RQ2 Baselines v1

- Added `src/modeling/baselines.py`: TF-IDF+LR and SVD+RF with time-based splits and reproducible outputs.
- Exports predictions, per-class metrics, and interpretability tables for RF (component terms + permutation importance).
- This creates inputs for fairness evaluation (RQ3) while meeting reproducibility standards.
