# 2025-09-03 — Reboot baseline & DB sanity

## Why

Tie the new repo to CE901 criteria (reproducibility, documentation, data integrity) and RQs 1–3: understand current categorisation data, quantify what’s available, and prepare for fairness measurement.

## What

- Created `equiTAG-RT` skeleton.
- Copied SQLite database and vendored legacy collectors.
- Added `src/setup/verify_database.py` to emit:
  - `reports/metrics/v0_db_profile.json`
  - `reports/metrics/v0_sample_videos.csv`

## How to run

python -m src.setup.verify_database
Outputs (expected)
JSON with tables: videos, video_tags, video_categories, category_status, collection_state, audit_terms (+ counts, indices, FKs).
CSV sample with 100 videos and aggregated tags.
Next
Lock config loader & seed policy; integrate protected-terms lexicon.
Build the RQ1 evidence pack: distributions, co-occurrence, overlap (titles vs tags), subgroup parity views.

### `docs/decisions/ADR-001-repo-structure.md`

```markdown
# ADR-001: Repository Structure & First Artifact Strategy

**Status:** Accepted — 2025-09-03

**Context.** We must meet CE901 standards: replicable pipeline, clear docs, fair ML evaluation. Baseline proof of data integrity is required before modeling.

**Decision.** Use a conventional, analysis-first structure:

- `src/setup` for integrity/bootstrap tools.
- `reports/metrics` as the single sink for machine artifacts (CSV/JSON).
- Vendored legacy collectors under `src/collect` to be refactored later.

**Consequences.**

- Easy narrative: Data → Integrity → EDA → Modeling → Fairness → Mitigation.
- Artifacts ready for dissertation figures/tables; minimal friction to proceed.

## Config & Reproducibility Module

- Added `src/utils/config_loader.py`:
  - Loads YAML config and resolves paths relative to project root.
  - Enforces **seed=75** and prints a consistent run header (root, config, DB, seed, device).
  - Picks device with safe defaults (MPS > CUDA > CPU) and applies precision settings.
- This aligns with CE901 marking (replicability, clarity) and supports RQs 2–4 by ensuring all modeling runs are traceable and comparable.
```
